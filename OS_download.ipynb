{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Download OS Open Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://realpython.com/python-download-file-from-url/#saving-downloaded-content-to-a-file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.dataquest.io/blog/python-api-tutorial/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import os\n",
    "import json\n",
    "from zipfile import ZipFile\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Get the current working directory\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add the main filepath\n",
    "main_file_path = \"//192.168.101.45/gis/_Projects/SpeciesDataPrep_20241209_48ad30\"\n",
    "\n",
    "# Location where data will be downloaded\n",
    "download_location = main_file_path + \"/commondata/userdata/Data_download/Downloads\"\n",
    "\n",
    "item_url_file = \"OS-API_URL.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change the current working directory\n",
    "os.chdir(download_location)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The OS-API_URL.csv file contains the column 'URL' with the URLs of the datasets that you want to download."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the csv file\n",
    "item_url_df = pd.read_csv(main_file_path + \"/commondata/userdata/Data_download/Source_files/\" + item_url_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "item_url_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a list of the item URLs\n",
    "url_list = item_url_df['URL'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download the data\n",
    "for url in url_list:\n",
    "    response = requests.get(url)\n",
    "    \n",
    "    url_split = url.split('/downloads?')\n",
    "    url_2 = url_split[0]\n",
    "    response_2 = requests.get(url_2)\n",
    "    \n",
    "    #https://stackoverflow.com/questions/16877422/whats-the-best-way-to-parse-a-json-response-from-the-requests-library\n",
    "    details = json.loads(response_2.text)\n",
    "    \n",
    "    if response.status_code == 200 and response_2.status_code == 200:\n",
    "        if 'area=GB' in url:\n",
    "            # https://www.w3schools.com/python/python_json.asp\n",
    "            name = details['id'] + \".zip\"\n",
    "            \n",
    "            with open(name, mode=\"wb\") as file:\n",
    "                file.write(response.content)\n",
    "        else:\n",
    "            # https://www.geeksforgeeks.org/python-extract-string-between-two-substrings/\n",
    "            url_3 = url.replace('?area=','*')\n",
    "            url_3 = url_3.replace('&format','*')\n",
    "            url_split_2 = url_3.split('*')\n",
    "            data_area = url_split_2[1]\n",
    "            \n",
    "            name = details['id'] + \"_\" + data_area + \".zip\"\n",
    "            \n",
    "            with open(name, mode=\"wb\") as file:\n",
    "                file.write(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# return all zip files at download_location as a list\n",
    "zip_list = []\n",
    "\n",
    "for file in os.listdir(download_location):\n",
    "     # check the files which end in .zip\n",
    "    if file.endswith(\".zip\"):\n",
    "        # add them to zip_list\n",
    "        zip_list.append(file)\n",
    "        \n",
    "zip_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove '.zip' extension from list items\n",
    "name_list = []\n",
    "\n",
    "for file_name in zip_list:\n",
    "    file_name_2 = file_name.replace(\".zip\", \"\")\n",
    "    name_list.append(file_name_2)\n",
    "    \n",
    "name_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unzip all zip files in download_location\n",
    "for (z_file, z_folder) in zip(zip_list, name_list):\n",
    "    with ZipFile(z_file, 'r') as z_object:\n",
    "        z_object.extractall(path=z_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete zip fiels\n",
    "for file in zip_list:\n",
    "    os.remove(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# magic command to clear all variables in the namespace\n",
    "# https://community.esri.com/t5/python-documents/arcgis-notebooks-in-arcgis-pro-3-1-faq/ta-p/1261606/jump-to/first-unread-message\n",
    "%reset -f"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ArcGISPro",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
